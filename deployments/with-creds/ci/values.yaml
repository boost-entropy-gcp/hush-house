postgresql:
  enabled: false

concourse:
  image: concourse/concourse-rc
  imageDigest: sha256:fa136abb336f2c2aed8d41d21b382d364c3387c24f3fdef15c720c292c9216d4

  secrets:
    teamAuthorizedKeys:
    - team: monitoring-hush-house
      key: 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDP5oT2CUJgRP55iNQD7NK4JzxRHSKstRXAV9EJw6O2Iaf9D9foHOaYNpgeIrRW4sbGvucLMhOBchp7yOhPTxn9jCUIGQttAWnFWW9SeHYOwPmbC30ggIIKmUQ/oWC6Xxou+KPotmqkKv+YyxNh8otTY5Sbz8VodEVqYR4hgXqunbSlcYJdMJqt6w0R319INdd898o6MbRrO5tj2I0ej8/Ct/n8Ijliawj3Mlm3g3w0O31C/Aj9jpEyvt+7JfcRWeJ1VcEnDsy4/UTqLh8P46LX/vzPQoPp54qrSaMcN7/1ylqn8XG5g+QWH4rmyJhH+0d1bt4v05M8b/UuHdXHgXVDMYyIFbvz2hRdhX7ZSHtP48e1B9t1S5Uo9gPG4D0jkWdMkpQf8/b4PXNF4nmBdKcWp9DqfYZOBMM17ZckOoWTnIORuZ/Xzgk1k11k0yAwRDxEksQuTSQexf5zxnku3ZediR5CwdbY9w4NiYB0DVCK7ktPi7Yg6RPrDgfALdB4vIg6jcWI3xw0ot2XVpZ2MGmUxL+ZlsymYncQ5p3pHRbznWbR2piniCa4rbE/KP0zFTDrNIMp2433nI0q5P1WbNuOQkL/XOIFkNr6f81ra9XJZWsV4Ytozivi5cg7hiPgCgxfUpnAQ+7NsSFYnV/gmmLjKUVCsdEwxqH5IxlxMYJqlQ=='

  postgresql:
    enabled: false

  web:
    annotations:
      rollingUpdate: "3"
    livenessProbe:
      # XXX(vito): for builds bigint migration - comment out when no longer
      # needed
      initialDelaySeconds: 600
    replicas: 2
    env:
    - name: CONCOURSE_X_FRAME_OPTIONS
      value: ""
    # TODO: use concourse.concourse.web.enablePipelineInstances instead once the chart is released
    - name: CONCOURSE_ENABLE_PIPELINE_INSTANCES
      value: "true"
      # The OTLP tracing stuff aren't on the latest chart yet so we're stting them as env vars
    - name: CONCOURSE_TRACING_SERVICE_NAME
      value: web
    - name: CONCOURSE_TRACING_OTLP_ADDRESS
      value: 127.0.0.1:55680
    - name: CONCOURSE_TRACING_OTLP_USE_TLS
      value: "false"
    nodeSelector: { cloud.google.com/gke-nodepool: generic-1 }
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                app: ci-web
                release: ci
    # NOTE: If you add sidecars make sure you set resources to maintain the QoS class of "guaranteed"
    sidecarContainers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.15.0
        args: ['--config=/etc/config/otelcol.yml']
        volumeMounts:
          - name: otelcol-config
            mountPath: /etc/config
        resources:
          limits:
            memory: 100Mi
            cpu: 500m
          requests:
            memory: 100Mi
            cpu: 500m
      - name: prom-storage-adapter
        image: wavefronthq/prometheus-storage-adapter
        resources:
          limits:
            memory: 100Mi
            cpu: 500m
          requests:
            memory: 100Mi
            cpu: 500m
        args:
          - -proxy=127.0.0.1
          - -proxy-port=2878
          - -listen=9000
          - -convert-paths=true
      - name: wavefront-proxy
        image: wavefronthq/proxy:9.2
        resources:
          limits:
            memory: 1Gi
            cpu: 500m
          requests:
            memory: 500Mi
            cpu: 500m
        env:
        - name: WAVEFRONT_URL
          value: "https://vmware.wavefront.com/api/"
        - name: WAVEFRONT_PROXY_ARGS
          # https://github.com/wavefrontHQ/wavefront-proxy/blob/master/pkg/etc/wavefront/wavefront-proxy/wavefront.conf.default
          value: |
            --prefix concourse
            --hostname=ci.concourse-ci.org
            --traceJaegerGrpcListenerPorts 14250
            --traceJaegerApplicationName Concourse
        - name: WAVEFRONT_TOKEN
          valueFrom:
            secretKeyRef:
              name: wavefront-proxy
              key: token
    additionalVolumes:
      - name: otelcol-config
        configMap:
          name: otelcol-config
    service:
      api:
        type: LoadBalancer
        loadBalancerIP: 34.69.51.78
      workerGateway:
        type: LoadBalancer
        loadBalancerIP: 34.69.51.78
    resources:
      requests:
        memory: 1Gi
        cpu: 1500m
      limits:
        memory: 2Gi
        cpu: 1500m

  persistence:
    worker:
      storageClass: ssd
      size: 750Gi

  worker:
    replicas: 8
    annotations:
      manual-update-revision: "1"
    terminationGracePeriodSeconds: 3600
    livenessProbe:
      periodSeconds: 60
      failureThreshold: 10
      timeoutSeconds: 45
    nodeSelector: { cloud.google.com/gke-nodepool: ci-workers }
    hardAntiAffinity: true
    resources:
      limits:   { cpu: 7500m, memory: 14Gi }
      requests: { cpu: 0m,    memory: 0Gi  }

  concourse:
    web:
      auth:
        mainTeam:
          localUser: admin
          github:
            team: concourse:Pivotal
        github:
          enabled: true
      # so pipeline-operator ended up with two permissions
      # - RerunJobBuild
      # - CheckResource
      # which will be granted to concourse:contributors for
      # operating PR pipeline
      configRBAC: |
        member:
        - AbortBuild
        - CreateJobBuild
        - PauseJob
        - UnpauseJob
        - ClearTaskCache
        - UnpinResource
        - SetPinCommentOnResource
        - CheckResourceWebHook
        - CheckResourceType
        - EnableResourceVersion
        - DisableResourceVersion
        - PinResourceVersion
        - PausePipeline
        - UnpausePipeline
      bindPort: 80
      clusterName: ci
      containerPlacementStrategy: limit-active-tasks
      maxActiveTasksPerWorker: 5
      streamingArtifactsCompression: zstd
      enableGlobalResources: true
      enableAcrossStep: true
      encryption: { enabled: true }
      externalUrl: https://ci.concourse-ci.org
      kubernetes:
        keepNamespaces: false
        enabled: false
        createTeamNamespaces: false
      metrics:
        attribute: "environment:ci"
      vault:
        enabled: true
        url: https://vault.vault.svc.cluster.local:8200
        sharedPath: shared
        authBackend: "cert"
        useCaCert: true
      letsEncrypt: { enabled: true, acmeURL: "https://acme-v02.api.letsencrypt.org/directory" }
      tls: { enabled: true, bindPort: 443 }
      prometheus:
        enabled: true

      postgres:
        host: 34.69.204.254
        database: atc
        sslmode: verify-ca
    worker:
      rebalanceInterval: 2h
      baggageclaim: { driver: overlay }
      healthcheckTimeout: 40s
      runtime: containerd
      containerd:
        networkPool: "10.254.0.0/16"
        maxContainers: "500"
        restrictedNetworks:
          - "169.254.169.254/32"

datadog:
  datadog:
    useDogStatsDSocketVolume: true

kubeStateMetrics:
  enabled: false
